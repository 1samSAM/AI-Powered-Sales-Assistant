import streamlit as st
import os
import sqlite3
import pandas as pd
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings

# Load environment variables from a .env file
load_dotenv()

# API Key for Groq
os.environ["GROQ_API_KEY"] = "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"
api_key = os.getenv("GROQ_API_KEY")
if not api_key:
    raise EnvironmentError("API key for Groq is not set. Please provide a valid key.")

# Initialize Groq Model
try:
    llm = ChatGroq(model="llama3-8b-8192", api_key=api_key)
    print("Groq model initialized successfully!")
except Exception as e:
    raise RuntimeError(f"Error initializing the Groq model: {e}")


# Paths
DB_PATH = "SalesCRM.db"
VECTOR_STORE_PATH = "vector_store_index"

# Load vector store
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

def load_vector_store(path, embeddings):
    """
    Load or create a FAISS vector store.

    Args:
        path (str): The file path for the FAISS vector store.
        embeddings: The embeddings model to use for creating/loading the store.

    Returns:
        FAISS: An instance of the FAISS vector store.
    """
    try:
        if os.path.exists(path):
            print("Loading existing vector store index...")
            return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)
        else:
            print("No existing index found. Initializing a new vector store...")
            return FAISS.from_texts([], embeddings)
    except Exception as e:
        raise RuntimeError(f"Failed to load or create vector store: {e}")


vector_store = load_vector_store(VECTOR_STORE_PATH, embeddings)


def fetch_customer_data(customer_name):
    """
    Fetch customer details and interaction history from the database.
    """
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    cursor.execute(
        """
        SELECT c.CustomerID, c.Name, c.Email, c.Phone, 
               ih.LastDealStatus, ih.InteractionDate, ih.Notes, ih.Sentiment, ih.Tone, ih.Intention
        FROM Customers c
        LEFT JOIN InteractionHistory ih ON c.CustomerID = ih.CustomerID
        WHERE c.Name = ?
        ORDER BY ih.InteractionDate DESC
        LIMIT 1
        """,
        (customer_name,),
    )
    customer = cursor.fetchone()
    conn.close()

    if customer:
        columns = [
            "CustomerID", "Name", "Email", "Phone", "LastDealStatus",
            "InteractionDate", "Notes", "Sentiment", "Tone", "Intention"
        ]
        return dict(zip(columns, customer))
    return None

def format_recommendation(recommendation):
    """
    Formats a single car recommendation in a readable and professional tone.
    """
    car_details = recommendation.page_content
    formatted = f"- {car_details.strip()}\n"
    return formatted

def recommend_deals(customer_data):
    """
    Generate recommendations based on customer data and product details.
    """
    sentiment = customer_data.get("Sentiment", "Neutral").lower()
    intention = customer_data.get("Intention", "Inquiry").lower()

    # Query the vector store for product recommendations based on the intention
    if intention == "purchase":
        query = "Budget-friendly petrol or diesel cars, manual transmission, first owner"
    elif intention == "inquiry":
        query = "Recent models from 2015 onwards"
    else:
        query = "General product recommendations"

    # Perform a search in the vector store
    results = vector_store.similarity_search(query, k=5)

    # Format recommendations
    recommendation_text = "Based on the customer's profile and preferences, I recommend the following cars:\n\n"
    for i, result in enumerate(results, start=1):
        # Extract the car details from the page content (assuming it's a description string)
        recommendation_text += f"{i}. {result.page_content.strip()}\n\n"
    
    return recommendation_text

# Few-shot examples for the prompt
examples = [
    HumanMessage("What should I suggest to John Doe?", name="example_user"),
    AIMessage(
        "",
        name="example_assistant",
        tool_calls=[{"name": "FetchCustomerData", "args": {"name": "John Doe"}, "id": "1"}]
    ),
    ToolMessage(
        """{'CustomerID': 1, 'Name': 'John Doe', 'Email': 'john.doe@example.com', 
        'Phone': '1234567890', 'LastDealStatus': 'Closed-Won', 'InteractionDate': '2024-12-20', 
        'Notes': 'Interested in budget cars', 'Sentiment': 'Positive', 'Tone': 'Happy', 'Intention': 'Purchase'}""",
        tool_call_id="1"
    ),
    AIMessage(
        "Based on John Doe's profile, I suggest budget-friendly manual cars. Here's the recommendation:\n- Car A\n- Car B\n",
        name="example_assistant",
    ),
]

# Define system behavior
system = """You are a conversational sales assistant. Your tasks are:
1. Ask for the customer's name.
2. Fetch the customer's information from the database.
3. Generate personalized recommendations based on customer data and product details.
4. Present recommendations in a professional and friendly tone."""

# Create prompt template
few_shot_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        *examples,
        ("human", "{query}"),
    ]
)

def run_sales_assistant():
    """
    Main function for interacting with the customer.
    """
    customer_name = input("Hello! Please provide your name: ")
    customer_data = fetch_customer_data(customer_name)

    if not customer_data:
        print(f"No data found for customer '{customer_name}'. Please check the name.")
        return

    # Generate recommendation
    recommendations = recommend_deals(customer_data)

    # Create LLM query with the recommendations
    query = f"Customer Profile: {customer_data}. Recommendation Text: {recommendations}."
    chain = {"query": RunnablePassthrough()} | few_shot_prompt | llm
    response = chain.invoke({"query": query})
    print("\nAssistant:", response)

# Streamlit app
st.title("AI Sales Assistant")
st.sidebar.title("Customer Query")

# Input for customer name
customer_name = st.text_input("Enter the customer's name:", placeholder="e.g., Jane Smith")

if st.button("Get Recommendations"):
    if customer_name.strip():
        customer_data = fetch_customer_data(customer_name)
        if customer_data:
            recommendations = recommend_deals(customer_data)
            query = f"Customer Profile: {customer_data}. Recommendation Text: {recommendations}."
            chain = {"query": RunnablePassthrough()} | ChatPromptTemplate.from_messages([
                ("system", "You are a professional sales assistant."),
                ("human", "{query}")
            ]) | llm
            response = chain.invoke({"query": query})
            st.markdown(f"### Recommendations for {customer_name}:")
            st.markdown(recommendations)
            st.markdown(f"### Assistant Response:\n{response}")
        else:
            st.warning(f"No data found for customer '{customer_name}'. Please check the name.")
    else:
        st.warning("Please enter a valid customer name.")


if __name__ == "__main__":
    run_sales_assistant()





import os
from dotenv import load_dotenv
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
load_dotenv(dotenv_path="config.env")

# Set Hugging Face API key
os.environ["HUGGINGFACE_API_KEY"] = os.getenv("HUGGINGFACE_API_KEY")

# Initialize embeddings model
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

# Initialize FAISS vector store globally (empty at the start)
vector_store = None

def ingest_product_data(xlsx_path: str):
    """
    Ingest product data from an XLSX file and add it to the FAISS vector store.
    Args:
        xlsx_path (str): Path to the XLSX file containing product data.
    """
    global vector_store

    try:
        # Read product data from Excel file
        product_data = pd.read_excel(xlsx_path)
        
        # Required columns
        required_columns = [
            "Name", "Location", "Year", "Kilometers_Driven", "Fuel_Type", 
            "Transmission", "Owner_Type", "Mileage", "Engine", "Power", 
            "Seats", "Price"
        ]
        
        # Validate required columns
        for col in required_columns:
            if col not in product_data.columns:
                raise KeyError(f"Missing required column: {col}")
        
        # Generate text representations
        product_texts = product_data.apply(
            lambda row: (
                f"Name: {row['Name']} | Location: {row['Location']} | Year: {row['Year']} | "
                f"Kilometers Driven: {row['Kilometers_Driven']} | Fuel Type: {row['Fuel_Type']} | "
                f"Transmission: {row['Transmission']} | Owner Type: {row['Owner_Type']} | "
                f"Mileage: {row['Mileage']} | Engine: {row['Engine']} | Power: {row['Power']} | "
                f"Seats: {row['Seats']} | Price: {row['Price']} lakhs"
            ),
            axis=1
        ).tolist()
        
        # Add texts to FAISS vector store
        vector_store = FAISS.from_texts(product_texts, embeddings)
        print(f"Indexed {len(product_texts)} product records.")
    except FileNotFoundError:
        print(f"Error: File '{xlsx_path}' not found. Please check the path and try again.")
    except KeyError as e:
        print(f"Error: {e}")
    except Exception as e:
        print(f"An unexpected error occurred during ingestion: {e}")

def save_index(index_file: str):
    """
    Save the FAISS vector store index to a file.
    Args:
        index_file (str): Path to save the index file.
    """
    try:
        if vector_store:
            vector_store.save_local(index_file)
            print(f"Index successfully saved to '{index_file}'.")
        else:
            print("No vector store to save.")
    except Exception as e:
        print(f"Error saving index: {e}")

def load_index(index_file: str):
    """
    Load the vector store index from a file.
    Args:
        index_file (str): Path to the saved index file.
    """
    try:
        # Enable dangerous deserialization explicitly
        vector_store = FAISS.load_local(index_file, embeddings, allow_dangerous_deserialization=True)
        print(f"Index successfully loaded from '{index_file}'.")
        return vector_store
    except Exception as e:
        print(f"Error loading index: {e}")
        return None

if __name__ == "__main__":
    # Paths for data and index
    product_xlsx = "product_details.xlsx"  # Replace with your Excel file path
    index_output = "vector_store_index"   # Desired FAISS index file name

    # Ingest data and save index
    print("Starting product data ingestion...")
    ingest_product_data(product_xlsx)
    
    print("Saving the vector store index...")
    save_index(index_output)
    
    # Load the index
    print("Loading the vector store index...")
    loaded_vector_store = load_index(index_output)

